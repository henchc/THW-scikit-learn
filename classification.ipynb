{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this unit, we'll explore how to use `scikit-learn` for text classification. We'll be using short texts collected from the [Universal Periodic Review](https://en.wikipedia.org/wiki/Universal_Periodic_Review), an international human rights mechanism. Each of these texts have an attached *label* or *labels* that pertain to the human rights issue that concerned in the text.\n",
    "\n",
    "From these texts, we're going to estimate a supervised model that tries to guess the label(s) from the text data. Note that this is a *multilabel* classification problem, because each text may have more than one label, or no label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import random\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics, tree, cross_validation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import RandomizedLogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import coverage_error\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load and PreProcess\n",
    "\n",
    "We'll first load in a csv file that contains our texts and their corresponding label(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in full csv\n",
    "recs = []\n",
    "with open('data/upr.csv','r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        recs.append(row)\n",
    "print(len(recs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turn labels into a list\n",
    "for i in recs:\n",
    "    issues = i['Issue'].split(',')\n",
    "    i['Issue'] = [x for x in issues if x != 'Other' and x != 'General']       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove texts with no label\n",
    "rec_sub = [i for i in recs if i['Issue']]\n",
    "print(\"Number of recs:\", len(rec_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turn into a dataframe\n",
    "data = DataFrame(rec_sub)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract text and label data\n",
    "text = data['Text'].values\n",
    "labels = data['Issue'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to \"binarize\" the labels, meaning that we transform it from a list of labels into an array of binary indicators: the one, i.e. the non zero elements, corresponds to the subset of labels. For instance, an array such as `np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])` represents label 0 in the first sample, labels 1 and 2 in the second sample, and no labels in the third sample. \n",
    "\n",
    "The `MultiLabelBinarizer` transformer can be used to convert between a collection of collections of labels and the indicator format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# binarize labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels_binary = mlb.fit_transform(labels)\n",
    "print(labels_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to extract our `X_train, X_test, y_train, y_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get training + test data\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    text, labels_binary, test_size=0.2, random_state=40)\n",
    "print(\"Number of training data observations:\", len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get target (label) names\n",
    "label_names = list(mlb.classes_)\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Pipelines\n",
    "\n",
    "Machine learning often involves a fixed sequence of steps for processing the data, for example feature selection, normalization and classification. \n",
    "\n",
    "Scikit-learn includes a [Pipeline](http://scikit-learn.org/stable/modules/pipeline.html) structure to help with this. Pipelines serve 2 purposes:\n",
    "\n",
    "1. **Convenience:** You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "2. **Joint parameter selection**: You can grid search over parameters of all estimators in the pipeline at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a pipeline with Support Vector Machines\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', OneVsRestClassifier(LinearSVC(random_state=0)))\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit using pipeline\n",
    "clf = text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Predicting\n",
    "\n",
    "Now we can use our trained model to predict the held-out \"test\" set. Better yet, there's no need to explicitely extract features or preprocess the data, since it uses the same pipeline as the training sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "predicted = clf.predict(X_test)\n",
    "clf.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mean agreement\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc, label in zip(list(X_test[:50]), predicted[:50]):\n",
    "    print('%r => %s' % (doc, \", \".join(list(np.array(label_names)[label==1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print metrics\n",
    "print(metrics.classification_report(y_test, predicted,\n",
    "    target_names=label_names)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Cross Validation and Grid Search\n",
    "\n",
    "`scikit-learn` is used differently from person to person depending on the task. It allows the user to build a basic single model from scratch, but also includes a grid-search function and cross-validation for more sophisticated exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## cross validation\n",
    "scores = cross_validation.cross_val_score(\n",
    "   text_clf, text, labels_binary, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## grid search\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (1,3)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Whare the best parameters?\n",
    "best_parameters, score, _ = max(gs_clf.grid_scores_, key=lambda x: x[1])\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_clf.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML on top of sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` itself does not include optimization algorithms for model parameters, but we will discuss two libraries, `auto-sklearn` and `TPOT`, which do.\n",
    "\n",
    "AutoML packages still require preparing and formatting the data as we've shown in preprocessing steps. You will hand off the prepped `X_train, y_train, X_test, y_test` arrays to the AutoML package, which will optimize a model and its parameters. Most arguments for AutoML have to do with the size of the desired model, the time to search for the best model, and where the model should be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) [auto-sklearn](http://automl.github.io/auto-sklearn/stable/) (Bayesian optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "import sklearn.cross_validation\n",
    "import sklearn.metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "\n",
    "X = text\n",
    "y = labels_binary\n",
    "X_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(X, y, random_state=1)\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(X)\n",
    "X_train = tfidf.transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "\n",
    "automl_cl = AutoSklearnClassifier()  # time_left_for_this_task=100\n",
    "automl_cl.fit(X_train, y_train)\n",
    "y_hat = automl_cl.predict(X_test)\n",
    "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "automl_r.show_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "automl_r.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "automl_r.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) [TPOT](https://github.com/rhiever/tpot) (genetic algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: TPOT does not yet support multi-label classification, but is adding features quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_mnist_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
