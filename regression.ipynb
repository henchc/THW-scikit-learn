{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regression in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main obstacles to working with regression in scikit-learn:\n",
    "\n",
    "1. Selecting the proper model\n",
    "2. Understanding the proper input data type, and scikit-learn syntax\n",
    "3. Testing and validation\n",
    "\n",
    "Today we will touch on all three, but focus on number 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Dataset and prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, we will use a boston housing dataset, which comes with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to follow along in other tutorials in the scikit-learn documentation, you will need to know the data structures used as inputs to the models. Let'see what's in the boston dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description will tell us more about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boston.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are working on predicitng median value of a home from 506 observations, and 13 covariates including crime rate, lot size, industry/commercial proportion, presence of the Charles River, nitric oxide concentration, rooms per dwelling, units built before 1940, distance to employment centers, access to highways, tax rate, school proxy, black population, and status. To get the variable names we can ask for them in the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(boston.feature_names)\n",
    "print()\n",
    "print(type(boston.feature_names))\n",
    "print()\n",
    "print(len(boston.feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the input is a numpy array of strings for the variable labels. To get the variable data, we ask the dictionary for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(boston.data)\n",
    "print()\n",
    "print(type(boston.data))\n",
    "print()\n",
    "print(len(boston.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a numpy array, inside of which there is a separate array for each observation (all 506 for each hous, *not* 13 for each variable). Each inner array *must* lineup with the order of the variables *and* all other arrays. **ORDER MATTERS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target, or *y* is accessed in the dictionary as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(boston.target)\n",
    "print()\n",
    "print(type(boston.target))\n",
    "print()\n",
    "print(len(boston.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target array is only one dimension, lined up in order with the with the observations in the data array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're familiar with the input data, we need to split it up for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n",
    "                                                    train_size=0.75, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 75% of the data as training data, and 25% of the data as testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(X_train), len(y_train))\n",
    "print()\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, as soon as you have `X_train`, `X_test`, `y_train`, and `y_test`, everything else is just a matter of choosing parameters for whichever model you choose. But this should not be trivialized, selecting models and that model's parameters is *very* important. While we will not cover it here, you should always select the model and parameters best suited for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Building models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax in scikit-learn does not change for each model, only the parameters. Examples of various models are given below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a basic OLS linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lin_reg = linear_model.LinearRegression(n_jobs=1)  # CPUs to use)\n",
    "\n",
    "model = lin_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM - Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "ridge_reg = linear_model.Ridge(alpha=1.0,  # regularization\n",
    "                               normalize=True,  # normalize X regressors\n",
    "                               solver='auto')  # options = ‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag'\n",
    "\n",
    "model = ridge_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM - Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elastic_reg = linear_model.ElasticNet(alpha=1.0,  # penalty, 0 is OLS \n",
    "                               random_state=None,\n",
    "                               selection='cyclic')  # or 'random', which converges faster\n",
    "\n",
    "model = elastic_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "sv_reg = svm.SVR(kernel='linear',  # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’\n",
    "                 degree=3,  # only used for 'poly' above\n",
    "                 gamma='auto',  # kernal coeff, default auto is 1/n_features\n",
    "                 C=1.0)\n",
    "\n",
    "model = sv_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbors regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "knn_reg = neighbors.KNeighborsRegressor(n_neighbors=5,\n",
    "                                        weights='uniform',  # ‘distance’ weights points by inverse of their distance\n",
    "                                        algorithm='auto',  # out of ‘ball_tree’, ‘kd_tree’, ‘brute’\n",
    "                                        leaf_size=30)  # for tree algorithms\n",
    "\n",
    "model = knn_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "rf_reg = ensemble.RandomForestRegressor(n_estimators=10,  # number of trees\n",
    "                                        criterion='mse',  # how to measure fit\n",
    "                                        max_depth=None,  # how deep tree nodes can go\n",
    "                                        min_samples_split=2,  # samples needed to split node\n",
    "                                        min_samples_leaf=1,  # samples needed for a leaf\n",
    "                                        min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n",
    "                                        max_features='auto',  # max feats\n",
    "                                        max_leaf_nodes=None,  # max nodes\n",
    "                                        n_jobs=1)  # how many to run parallel\n",
    "\n",
    "model = rf_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting - AdaBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ab_reg = ensemble.AdaBoostRegressor(base_estimator=None,  # default is DT \n",
    "                                    n_estimators=50,  # number to try before stopping\n",
    "                                    learning_rate=1.0,  # decrease influence of each additional estimator\n",
    "                                    loss='linear')  # also ‘square’, ‘exponential’\n",
    "\n",
    "\n",
    "model = ab_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Regression (*new in scikit-learn .18*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neural_network\n",
    "nn_reg = neural_network.MLPRegressor(hidden_layer_sizes=(7, ),  # neurons in ith layer\n",
    "                                     activation='relu',  # activation function, also 'identity', 'logistic', 'tanh'\n",
    "                                     solver='adam',  # solves weight optimization, also ‘lbfgs’, ‘sgd’, ‘adam’\n",
    "                                     alpha=0.0001,  # L2 penalty\n",
    "                                     batch_size='auto',  # auto is 200\n",
    "                                     learning_rate='constant',  # schedule for weight updates, also ‘invscaling’, ‘adaptive’\n",
    "                                     learning_rate_init=0.001,  # lr initial value for sgd and adam\n",
    "                                     max_iter=200,  # max iterations\n",
    "                                     momentum=0.9,  # for gradient descent, only sgd\n",
    "                                     early_stopping=False,  # if validation not improving\n",
    "                                     validation_fraction=0.1,  # validation subset\n",
    "                                     beta_1=0.9,  # decay for adam first momentum\n",
    "                                     beta_2=0.999)  # decay for adam second momentum\n",
    "\n",
    "model = nn_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our division of training and testing data was naïve, ideally you will do k-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(sv_reg, boston.data, boston.target, cv=3)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print (np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML on top of sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoML packages still require preparing and formatting the data as we've shown in preprocessing steps. You will hand off the prepped `X_train, y_train, X_test, y_test` arrays to the AutoML package, which will optimize a model and its parameters. Most arguments for AutoML have to do with the size of the desired model, the time to search for the best model, and where the model should be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) [auto-sklearn](http://automl.github.io/auto-sklearn/stable/) (Bayesian optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from autosklearn.regression import AutoSklearnRegressor\n",
    "import sklearn.cross_validation\n",
    "import sklearn.metrics\n",
    "\n",
    "automl_r = AutoSklearnRegressor()  # time_left_for_this_task=100\n",
    "automl_r.fit(X_train, y_train)\n",
    "y_hat = automl_r.predict(X_test)\n",
    "print(\"R2 score\", sklearn.metrics.r2_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get final ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(automl_r.show_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get iteration scores:\n",
    "From docs: `(list of named tuples) Contains scores for all parameter combinations in param_grid. Each entry corresponds to one parameter setting. Each named tuple has the attributes: * parameters, a dict of parameter settings * mean_validation_score, the mean score over the cross-validation folds * cv_validation_scores, the list of scores for each fold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "automl_r.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation results:\n",
    "\n",
    "From docs: `(dict of numpy (masked) ndarrays) A dict with keys as column headers and values as columns, that can be imported into a pandas DataFrame. This attribute is a backward port to already support the advanced output of scikit-learn 0.18. Not all keys returned by scikit-learn are supported yet.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "automl_r.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) [TPOT](https://github.com/rhiever/tpot) (genetic algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a model and parameters can be a difficult task. While you should at least conceptually understand the process and reason for your selection, a new package called `TPOT` uses genetic algorithms to help you select a model and its parameters by just feeding it the `X` and `y`. It will even write the scikit-learn code for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "\n",
    "tpot = TPOTRegressor(generations=5, population_size=20, verbosity=2)  # generations for optimization, , pop size is models\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_boston_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
