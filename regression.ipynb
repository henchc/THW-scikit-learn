{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main obstacles to working with regression in scikit-learn:\n",
    "\n",
    "1. Selecting the proper model\n",
    "2. Understanding the proper input data type, and scikit-learn syntax\n",
    "3. Testing and validation\n",
    "\n",
    "Today we will touch on all three, but focus on number 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, we will use a boston housing dataset, which comes with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to follow along in other tutorials in the scikit-learn documentation, you will need to know the data structures used as inputs to the models. Let'see what's in the boston dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description will tell us more about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boston.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are working on predicitng median value of a home from 506 observations, and 13 covariates including crime rate, lot size, industry/commercial proportion, presence of the Charles River, nitric oxide concentration, rooms per dwelling, units built before 1940, distance to employment centers, access to highways, tax rate, school proxy, black population, and status. To get the variable names we can ask for them in the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(boston.feature_names)\n",
    "print()\n",
    "print(type(boston.feature_names))\n",
    "print()\n",
    "print(len(boston.feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the input is a numpy array of strings for the variable labels. To get the variable data, we ask the dictionary for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(boston.data)\n",
    "print()\n",
    "print(type(boston.data))\n",
    "print()\n",
    "print(len(boston.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a numpy array, inside of which there is a separate array for each observation (all 506 for each hous, *not* 13 for each variable). Each inner array *must* lineup with the order of the variables *and* all other arrays. **ORDER MATTERS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target, or *y* is accessed in the dictionary as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(boston.target)\n",
    "print()\n",
    "print(type(boston.target))\n",
    "print()\n",
    "print(len(boston.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target array is only one dimmension, lined up in order with the with the observations in the data array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're familiar with the input data, we need to split it up for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n",
    "                                                    train_size=0.75, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 75% of the data as training data, and 25% of the data as testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(X_train), len(y_train))\n",
    "print()\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, as soon as you have `X_train`, `X_test`, `y_train`, and `y_test`, everything else is just a matter of choosing parameters for whichever model you choose. But this should not be trivialized, selecting models and that model's parameters is *very* important. While we will not cover it here, you should always select the model and parameters best suited for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a basic linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lin_reg = linear_model.LinearRegression(fit_intercept=True,\n",
    "                                          normalize=False,\n",
    "                                          copy_X=True,\n",
    "                                          n_jobs=1)\n",
    "\n",
    "model = lin_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "ridge_reg = linear_model.Ridge(alpha=1.0,\n",
    "                               fit_intercept=True,\n",
    "                               normalize=False,\n",
    "                               copy_X=True,\n",
    "                               max_iter=None,\n",
    "                               tol=0.001, solver='auto',\n",
    "                               random_state=None)\n",
    "\n",
    "model = ridge_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elastic_reg = linear_model.ElasticNet(alpha=1.0,\n",
    "                               fit_intercept=True,\n",
    "                               normalize=False,\n",
    "                               precompute=False,\n",
    "                               copy_X=True,\n",
    "                               max_iter=1000,\n",
    "                               tol=0.0001, \n",
    "                               warm_start=False,\n",
    "                               positive=False,\n",
    "                               random_state=None,\n",
    "                               selection='cyclic')\n",
    "\n",
    "model = elastic_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "sv_reg = svm.SVR(kernel='rbf',\n",
    "                 degree=3,\n",
    "                 gamma='auto',\n",
    "                 coef0=0.0,\n",
    "                 tol=0.001,\n",
    "                 C=1.0,\n",
    "                 epsilon=0.1,\n",
    "                 shrinking=True,\n",
    "                 cache_size=200,\n",
    "                 verbose=False, \n",
    "                 max_iter=-1)\n",
    "\n",
    "model = sv_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbors regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "knn_reg = neighbors.KNeighborsRegressor(5, weights='distance')\n",
    "\n",
    "model = knn_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ab_reg = ensemble.AdaBoostRegressor(base_estimator=None, \n",
    "                                    n_estimators=50,\n",
    "                                    learning_rate=1.0,\n",
    "                                    loss='linear',\n",
    "                                    random_state=None)\n",
    "\n",
    "\n",
    "model = ab_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "rf_reg = ensemble.RandomForestRegressor(n_estimators=10,\n",
    "                                        criterion='mse',\n",
    "                                        max_depth=None, \n",
    "                                        min_samples_split=2, \n",
    "                                        min_samples_leaf=1,\n",
    "                                        min_weight_fraction_leaf=0.0,\n",
    "                                        max_features='auto',\n",
    "                                        max_leaf_nodes=None,\n",
    "                                        bootstrap=True,\n",
    "                                        oob_score=False,\n",
    "                                        n_jobs=1,\n",
    "                                        random_state=None,\n",
    "                                        verbose=0,\n",
    "                                        warm_start=False)\n",
    "\n",
    "model = rf_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neural_network\n",
    "nn_reg = neural_network.MLPRegressor(hidden_layer_sizes=(100, ),\n",
    "                            activation='relu',\n",
    "                            solver='adam',\n",
    "                            alpha=0.0001,\n",
    "                            batch_size='auto',\n",
    "                            learning_rate='constant',\n",
    "                            learning_rate_init=0.001,\n",
    "                            power_t=0.5,\n",
    "                            max_iter=200,\n",
    "                            shuffle=True,\n",
    "                            random_state=40, # reproducibility\n",
    "                            tol=0.0001,\n",
    "                            verbose=False,\n",
    "                            warm_start=False,\n",
    "                            momentum=0.9,\n",
    "                            nesterovs_momentum=True,\n",
    "                            early_stopping=False,\n",
    "                            validation_fraction=0.1,\n",
    "                            beta_1=0.9,\n",
    "                            beta_2=0.999,\n",
    "                            epsilon=1e-08)\n",
    "\n",
    "model = nn_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a model and parameters can be a difficult task. While you should at least conceptually understand the process and reason for your selection, a new package called `TPOT` uses genetic algorithms to help you select a model and its parameters by just feeding it the `X` and `y`. It will even write the scikit-learn code for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "\n",
    "tpot = TPOTRegressor(generations=5, population_size=20, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_boston_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
